

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>pusion.evaluation.evaluation_metrics module &mdash; pusion - Decision Fusion Framework  documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/pusion.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="pusion.model package" href="pusion.model.html" />
    <link rel="prev" title="pusion.evaluation.evaluation module" href="pusion.evaluation.evaluation.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html">
          

          
            
            <img src="_static/pusion_logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Install Pusion</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="pusion.html">API Reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="pusion.auto.html">pusion.auto package</a></li>
<li class="toctree-l2"><a class="reference internal" href="pusion.control.html">pusion.control package</a></li>
<li class="toctree-l2"><a class="reference internal" href="pusion.core.html">pusion.core package</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="pusion.evaluation.html">pusion.evaluation package</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="pusion.evaluation.evaluation.html">pusion.evaluation.evaluation module</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">pusion.evaluation.evaluation_metrics module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="pusion.model.html">pusion.model package</a></li>
<li class="toctree-l2"><a class="reference internal" href="pusion.util.html">pusion.util package</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="usage_and_examples.html">Usage and Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="about.html">About</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">pusion - Decision Fusion Framework</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="pusion.html">API Reference</a> &raquo;</li>
        
          <li><a href="pusion.evaluation.html">pusion.evaluation package</a> &raquo;</li>
        
      <li>pusion.evaluation.evaluation_metrics module</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/pusion.evaluation.evaluation_metrics.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-pusion.evaluation.evaluation_metrics">
<span id="pusion-evaluation-evaluation-metrics-module"></span><h1>pusion.evaluation.evaluation_metrics module<a class="headerlink" href="#module-pusion.evaluation.evaluation_metrics" title="Permalink to this headline">¶</a></h1>
<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multi_label_brier_score_micro">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multi_label_brier_score_micro</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multi_label_brier_score_micro" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the brier score for multi-label problems according to Brier 1950
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The micro brier score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multi_label_brier_score">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multi_label_brier_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multi_label_brier_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the brier score for multiclass problems according to Brier 1950
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The brier score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multiclass_brier_score">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multiclass_brier_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multiclass_brier_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the brier score for multi-label problems according to Brier 1950
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The brier score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.far">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">far</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_normal_class</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.far" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the false alarm rate for multiclass and multi-label problems.
FAR = (number of normal class samples incorrectly classified)/(number of all normal class samples) * 100
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:param pos_normal_class: the position of the ‘normal class’ in :param y_true and :param y_pred. Default is <cite>0</cite>
:return: The false alarm rate.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multiclass_fdr">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multiclass_fdr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multiclass_fdr" title="Permalink to this definition">¶</a></dt>
<dd><p>fault detection rate = (# correctly classified faulty samples) / (# all faulty samples) * 100
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The fault detection rate.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multilabel_subset_fdr">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multilabel_subset_fdr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multilabel_subset_fdr" title="Permalink to this definition">¶</a></dt>
<dd><p>fault detection rate = (# correctly classified faulty samples) / (# all faulty samples) * 100
In multilabel classification, the function considers the faulty subset, i. e., if the entire set
of predicted faulty labels for a sample strictly match with the true set of faulty labels.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The fault detection rate.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multilabel_minor_fdr">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multilabel_minor_fdr</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multilabel_minor_fdr" title="Permalink to this definition">¶</a></dt>
<dd><p>fault detection rate = (# correctly classified faulty samples) / (# all faulty samples) * 100
In multilabel classification, the function considers the faulty subset, i. e., if the entire set
of predicted faulty labels for a sample strictly match with the true set of faulty labels.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The fault detection rate.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multiclass_weighted_precision">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multiclass_weighted_precision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multiclass_weighted_precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the precision for a multiclass problem with a <cite>weighted</cite> average.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite>. Predicted labels or class assignments.
:return: The precision score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multi_label_weighted_precision">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multi_label_weighted_precision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multi_label_weighted_precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the precision for a multi-label problem with a <cite>weighted</cite> average.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The precision score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multiclass_class_wise_precision">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multiclass_class_wise_precision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">numpy.ndarray</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multiclass_class_wise_precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the precision for a multiclass problem with average <cite>None</cite>.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The precision score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multi_label_class_wise_precision">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multi_label_class_wise_precision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">numpy.ndarray</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multi_label_class_wise_precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the precision for a multi-label problem with average <cite>None</cite>.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The precision score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multiclass_recall">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multiclass_recall</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multiclass_recall" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the recall for a multiclass problem with average <cite>weighted</cite>.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite>. Predicted labels or class assignments.
:return: The recall score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multi_label_recall">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multi_label_recall</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multi_label_recall" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the recall for a multi-label problem with average <cite>weighted</cite>.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The recall score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multiclass_class_wise_recall">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multiclass_class_wise_recall</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">numpy.ndarray</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multiclass_class_wise_recall" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the recall for a multiclass problem with average <cite>None</cite>.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: Sequence of recall scores (for each class).</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multi_label_class_wise_recall">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multi_label_class_wise_recall</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">numpy.ndarray</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multi_label_class_wise_recall" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the recall for a multi-label problem with average <cite>None</cite>.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: Sequence of recall scores (for each class).</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multiclass_weighted_scikit_auc_roc_score">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multiclass_weighted_scikit_auc_roc_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multiclass_weighted_scikit_auc_roc_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the scikit auc roc score for a multi-label problem with average <cite>weighted</cite>.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The auc roc score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multi_label_weighted_pytorch_auc_roc_score">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multi_label_weighted_pytorch_auc_roc_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multi_label_weighted_pytorch_auc_roc_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the pytorch auc roc score for a multi-label problem with average <cite>weighted</cite>.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The auc roc score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multi_label_pytorch_auc_roc_score">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multi_label_pytorch_auc_roc_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multi_label_pytorch_auc_roc_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the pytorch auc roc score for a multi-label problem with average <cite>None</cite>.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The auc roc score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multiclass_class_wise_avg_precision">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multiclass_class_wise_avg_precision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multiclass_class_wise_avg_precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the class wise precision for a multiclass problem with average <cite>None</cite>.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The precision score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multiclass_weighted_avg_precision">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multiclass_weighted_avg_precision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multiclass_weighted_avg_precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the precision for a multiclass problem with average <cite>weighted</cite>.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The precision score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multiclass_auc_precision_recall_curve">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multiclass_auc_precision_recall_curve</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multiclass_auc_precision_recall_curve" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the class wise auc precision recall curve for a multiclass problem.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The aggregated auc precision recall curve class wise.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multiclass_weighted_pytorch_auc_roc">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multiclass_weighted_pytorch_auc_roc</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multiclass_weighted_pytorch_auc_roc" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the pytorch auc roc for a multiclass problem with average <cite>weighted</cite>.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The auc roc score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multiclass_pytorch_auc_roc">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multiclass_pytorch_auc_roc</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multiclass_pytorch_auc_roc" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the pytorch auc roc for a multiclass problem with average <cite>None</cite>.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The auc roc score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multi_label_ranking_avg_precision_score">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multi_label_ranking_avg_precision_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multi_label_ranking_avg_precision_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the label ranking based average precision score for a multi-label problem.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The precision score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multi_label_ranking_loss">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multi_label_ranking_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multi_label_ranking_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the label ranking loss for a multi-label problem.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The precision score.
:return: The loss.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multi_label_normalized_discounted_cumulative_gain">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multi_label_normalized_discounted_cumulative_gain</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multi_label_normalized_discounted_cumulative_gain" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the normalized discounted cumulative gain for a multi-label problem.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The gain.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multiclass_top_1_accuracy">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multiclass_top_1_accuracy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multiclass_top_1_accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the top-1 accuracy for a multiclass problem.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The accuracy score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multiclass_top_3_accuracy">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multiclass_top_3_accuracy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multiclass_top_3_accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the top-3 accuracy for a multiclass problem.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The accuracy score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multiclass_top_5_accuracy">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multiclass_top_5_accuracy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multiclass_top_5_accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the top-5 accuracy for a multiclass problem.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape (n_samples, n_classes)`. Predicted labels or class assignments.
:return: The accuracy score.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multiclass_log_loss">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multiclass_log_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multiclass_log_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>The logarithmic loss for a multiclass problem.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The loss.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.multi_label_log_loss">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">multi_label_log_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">numpy.ndarray</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">float</span></span></span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.multi_label_log_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>The logarithmic loss for a multi-label problem.
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: The loss.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.micro_precision">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">micro_precision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.micro_precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the micro precision, i.e. TP / (TP + FP).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.</p></li>
<li><p><strong>y_pred</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or
class assignments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The micro precision.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.micro_recall">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">micro_recall</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.micro_recall" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the micro recall, i.e.  TP / (TP + FN).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.</p></li>
<li><p><strong>y_pred</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or
class assignments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The micro recall.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.micro_f1">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">micro_f1</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.micro_f1" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the micro F1-score, i.e. 2 * (Precision * Recall) / (Precision + Recall).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.</p></li>
<li><p><strong>y_pred</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or
class assignments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The micro F1-score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.micro_f2">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">micro_f2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.micro_f2" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the micro F2-score (beta=2).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.</p></li>
<li><p><strong>y_pred</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or
class assignments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The micro F2-score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.micro_jaccard">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">micro_jaccard</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.micro_jaccard" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the micro Jaccard-score, i.e. TP / (TP + FP + FN).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.</p></li>
<li><p><strong>y_pred</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or
class assignments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The micro Jaccard-score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.macro_precision">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">macro_precision</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.macro_precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the macro precision, i.e. TP / (TP + FP).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.</p></li>
<li><p><strong>y_pred</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or
class assignments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The macro precision.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.macro_recall">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">macro_recall</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.macro_recall" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the macro recall, i.e.  TP / (TP + FN).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.</p></li>
<li><p><strong>y_pred</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or
class assignments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The macro recall.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.macro_f1">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">macro_f1</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.macro_f1" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the macro F1-score, i.e. 2 * (Precision * Recall) / (Precision + Recall).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.</p></li>
<li><p><strong>y_pred</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or
class assignments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The macro F1-score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.weighted_f1">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">weighted_f1</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.weighted_f1" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the macro F1-score, i.e. 2 * (Precision * Recall) / (Precision + Recall), weighted by the class support.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.</p></li>
<li><p><strong>y_pred</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or
class assignments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The weighted macro F1-score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.macro_f2">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">macro_f2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.macro_f2" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the macro F2-score (beta=2).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.</p></li>
<li><p><strong>y_pred</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or
class assignments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The macro F2-score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.macro_jaccard">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">macro_jaccard</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.macro_jaccard" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the macro Jaccard-score, i.e. TP / (TP + FP + FN).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.</p></li>
<li><p><strong>y_pred</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or
class assignments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The macro Jaccard-score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.weighted_jaccard">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">weighted_jaccard</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.weighted_jaccard" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the Jaccard-score for each label, and find their average, weighted by support, i. e., the number of true instances of each label instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.</p></li>
<li><p><strong>y_pred</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The macro Jaccard-score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.accuracy">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">accuracy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the accuracy, i.e. (TP + TN) / (TP + FP + FN + TN).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.</p></li>
<li><p><strong>y_pred</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or
class assignments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Accuracy.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.error_rate">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">error_rate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.error_rate" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the error rate, i. e. error_rate = 1-accuracy
:param y_true: <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.
:param y_pred: <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or class assignments.
:return: Error Rate of typ <cite>float</cite></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.balanced_multiclass_accuracy">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">balanced_multiclass_accuracy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.balanced_multiclass_accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the balanced accuracy, i.e. (Precision + Recall) / 2.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.</p></li>
<li><p><strong>y_pred</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or
class assignments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Accuracy.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.mean_multilabel_confusion_matrix">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">mean_multilabel_confusion_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.mean_multilabel_confusion_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the normalized mean confusion matrix across all classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.</p></li>
<li><p><strong>y_pred</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or
class assignments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><cite>numpy.array</cite> of shape <cite>(n_classes, n_classes)</cite>. Normalized mean confusion matrix.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.mean_confidence">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">mean_confidence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.mean_confidence" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the mean confidence for continuous multiclass and multilabel classification outputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. True class assignments.</p></li>
<li><p><strong>y_pred</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>. Predicted class assignments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Mean confidence.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.hamming">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">hamming</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.hamming" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the average Hamming Loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.</p></li>
<li><p><strong>y_pred</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or
class assignments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Average Hamming Loss.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.log">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the Logistic Loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. True labels or class assignments.</p></li>
<li><p><strong>y_pred</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Predicted labels or
class assignments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Logistic Loss.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.cohens_kappa">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">cohens_kappa</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.cohens_kappa" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the Cohen’s Kappa annotator agreement score according to <a class="footnote-reference brackets" href="#cohen1960coefficient" id="id1">1</a>.</p>
<p><dl class="footnote brackets">
<dt class="label" id="cohen1960coefficient"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Jacob Cohen. A coefficient of agreement for nominal scales. <em>Educational and psychological measurement</em>, 20(1):37–46, 1960.</p>
</dd>
</dl>
</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y1</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Labels or class assignments.</p></li>
<li><p><strong>y2</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples,)</cite> or <cite>(n_samples, n_classes)</cite>. Labels or class assignments.</p></li>
<li><p><strong>labels</strong> – <cite>list</cite> of all possible labels.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Cohen’s Kappa score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.pairwise_cohens_kappa">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">pairwise_cohens_kappa</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decision_tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.pairwise_cohens_kappa" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the average of pairwise Cohen’s Kappa scores over all multiclass decision outputs.
E.g., for 3 classifiers <cite>(0,1,2)</cite>, the agreement score is calculated for classifier tuples <cite>(0,1)</cite>, <cite>(0,2)</cite> and
<cite>(1,2)</cite>. These scores are then averaged over all 3 classifiers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>decision_tensor</strong> – <cite>numpy.array</cite> of shape <cite>(n_classifiers, n_samples, n_classes)</cite>.
Tensor of crisp multiclass decision outputs by different classifiers per sample.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Pairwise (averages) Cohen’s Kappa score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.correlation">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">correlation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.correlation" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the correlation score for decision outputs of two classifiers according to Kuncheva
<a class="footnote-reference brackets" href="#kuncheva2014combining" id="id2">2</a>.</p>
<p><dl class="footnote brackets">
<dt class="label" id="kuncheva2014combining"><span class="brackets">2</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id4">2</a>)</span></dt>
<dd><p>Ludmila I Kuncheva. <em>Combining pattern classifiers: methods and algorithms</em>. John Wiley &amp; Sons, 2014.</p>
</dd>
</dl>
</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y1</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Crisp multiclass decision outputs by the first classifier.</p></li>
<li><p><strong>y2</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Crisp multiclass decision outputs by the second classifier.</p></li>
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Matrix of crisp class assignments which are considered as true.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Correlation score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.q_statistic">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">q_statistic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.q_statistic" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the Q statistic score for decision outputs of two classifiers according to Yule
<a class="footnote-reference brackets" href="#udny1900association" id="id3">3</a>.</p>
<p><dl class="footnote brackets">
<dt class="label" id="udny1900association"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>G Udny Yule. On the association of attributes in statistics: with illustrations from the material of the childhood society, &amp;c. <em>Philosophical Transactions of the Royal Society of London Series A</em>, 194:257–319, 1900.</p>
</dd>
</dl>
</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y1</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Crisp multiclass decision outputs by the first classifier.</p></li>
<li><p><strong>y2</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Crisp multiclass decision outputs by the second classifier.</p></li>
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Matrix of crisp class assignments which are considered as true.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Correlation score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.kappa_statistic">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">kappa_statistic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.kappa_statistic" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the kappa score for decision outputs of two classifiers according to Kuncheva
<a class="footnote-reference brackets" href="#kuncheva2014combining" id="id4">2</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y1</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Crisp multiclass decision outputs by the first classifier.</p></li>
<li><p><strong>y2</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Crisp multiclass decision outputs by the second classifier.</p></li>
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Matrix of crisp class assignments which are considered as true.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Kappa score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.disagreement">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">disagreement</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.disagreement" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the disagreement for decision outputs of two classifiers, i.e. the percentage of samples which are
correctly classified by exactly one of the classifiers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y1</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Crisp multiclass decision outputs by the first classifier.</p></li>
<li><p><strong>y2</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Crisp multiclass decision outputs by the second classifier.</p></li>
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Matrix of crisp class assignments which are considered as true.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Disagreement score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.double_fault">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">double_fault</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.double_fault" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the double fault for decision outputs of two classifiers, i.e. the percentage of samples which are
misclassified by both classifiers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y1</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Crisp multiclass decision outputs by the first classifier.</p></li>
<li><p><strong>y2</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Crisp multiclass decision outputs by the second classifier.</p></li>
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Matrix of crisp class assignments which are considered as true.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Double fault score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.abs_correlation">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">abs_correlation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.abs_correlation" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the absolute correlation score for decision outputs of two classifiers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y1</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Crisp multiclass decision outputs by the first classifier.</p></li>
<li><p><strong>y2</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Crisp multiclass decision outputs by the second classifier.</p></li>
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Matrix of crisp class assignments which are considered as true.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Correlation score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.abs_q_statistic">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">abs_q_statistic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.abs_q_statistic" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the absolute Q statistic score for decision outputs of two classifiers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y1</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Crisp multiclass decision outputs by the first classifier.</p></li>
<li><p><strong>y2</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Crisp multiclass decision outputs by the second classifier.</p></li>
<li><p><strong>y_true</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Matrix of crisp class assignments which are considered as true.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Correlation score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.pairwise_correlation">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">pairwise_correlation</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decision_tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true_assignments</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.pairwise_correlation" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the average of the pairwise absolute correlation scores over all decision outputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>decision_tensor</strong> – <cite>numpy.array</cite> of shape <cite>(n_classifiers, n_samples, n_classes)</cite>.
Tensor of crisp multiclass decision outputs by different classifiers per sample.</p></li>
<li><p><strong>true_assignments</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Matrix of crisp class assignments which are considered as true.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Pairwise correlation score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.pairwise_q_statistic">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">pairwise_q_statistic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decision_tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true_assignments</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.pairwise_q_statistic" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the average of the pairwise absolute Q-statistic scores over all decision outputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>decision_tensor</strong> – <cite>numpy.array</cite> of shape <cite>(n_classifiers, n_samples, n_classes)</cite>.
Tensor of crisp multiclass decision outputs by different classifiers per sample.</p></li>
<li><p><strong>true_assignments</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Matrix of crisp class assignments which are considered as true.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Pairwise correlation score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.pairwise_kappa_statistic">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">pairwise_kappa_statistic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decision_tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true_assignments</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.pairwise_kappa_statistic" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the average of pairwise Kappa scores over all decision outputs.
Multilabel class assignments are transformed to equivalent multiclass class assignments.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>decision_tensor</strong> – <cite>numpy.array</cite> of shape <cite>(n_classifiers, n_samples, n_classes)</cite>.
Tensor of crisp multiclass decision outputs by different classifiers per sample.</p></li>
<li><p><strong>true_assignments</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Matrix of crisp class assignments which are considered as true.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Pairwise kappa score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.pairwise_disagreement">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">pairwise_disagreement</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decision_tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true_assignments</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.pairwise_disagreement" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the average of pairwise disagreement scores over all decision outputs.
Multilabel class assignments are transformed to equivalent multiclass class assignments.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>decision_tensor</strong> – <cite>numpy.array</cite> of shape <cite>(n_classifiers, n_samples, n_classes)</cite>.
Tensor of crisp multiclass decision outputs by different classifiers per sample.</p></li>
<li><p><strong>true_assignments</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Matrix of crisp class assignments which are considered as true.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Pairwise disagreement score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.pairwise_double_fault">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">pairwise_double_fault</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decision_tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">true_assignments</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.pairwise_double_fault" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the average of pairwise double fault scores over all decision outputs.
Multilabel class assignments are transformed to equivalent multiclass class assignments.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>decision_tensor</strong> – <cite>numpy.array</cite> of shape <cite>(n_classifiers, n_samples, n_classes)</cite>.
Tensor of crisp multiclass decision outputs by different classifiers per sample.</p></li>
<li><p><strong>true_assignments</strong> – <cite>numpy.array</cite> of shape <cite>(n_samples, n_classes)</cite>.
Matrix of crisp class assignments which are considered as true.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Pairwise double fault score.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="pusion.evaluation.evaluation_metrics.pairwise_euclidean_distance">
<span class="sig-prename descclassname"><span class="pre">pusion.evaluation.evaluation_metrics.</span></span><span class="sig-name descname"><span class="pre">pairwise_euclidean_distance</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decision_tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#pusion.evaluation.evaluation_metrics.pairwise_euclidean_distance" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the average of pairwise euclidean distance between decision matrices for the given classifiers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>decision_tensor</strong> – <cite>numpy.array</cite> of shape <cite>(n_classifiers, n_samples, n_classes)</cite>.
Tensor of crisp multiclass decision outputs by different classifiers per sample.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Pairwise euclidean distance.</p>
</dd>
</dl>
</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="pusion.model.html" class="btn btn-neutral float-right" title="pusion.model package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="pusion.evaluation.evaluation.html" class="btn btn-neutral float-left" title="pusion.evaluation.evaluation module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, Admir Obralija, Yannick Wilhelm. Institute for Parallel and Distributed Systems, University of Stuttgart, Germany.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>